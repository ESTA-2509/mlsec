{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "# !pip3 uninstall numpy\n",
    "!pip3 install --upgrade numpy==2.0.0\n",
    "!pip3 install pandas\n",
    "!pip3 install scikit-learn mlflow seaborn shap\n",
    "!pip3 install bayesian-optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset\n",
    "Because test dataset not have label, we must split train dataset to 2 parts. One for train and one for validate. We just do this on the first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "# df = pd.read_csv('data/data.csv')\n",
    "# train_test_data, validate_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "# train_test_data.to_csv('data/train_.csv', index=False, header=True)\n",
    "# validate_data.to_csv('data/validate.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check some information of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset column\n",
      "Index(['ID', 'flow_duration', 'Header_Length', 'Protocol type', 'Duration',\n",
      "       'Rate', 'Srate', 'Drate', 'fin_flag_number', 'syn_flag_number',\n",
      "       'rst_flag_number', 'psh_flag_number', 'ack_flag_number',\n",
      "       'ece_flag_number', 'cwr_flag_number', 'ack_count', 'syn_count',\n",
      "       'fin_count', 'urg_count', 'rst_count', 'HTTP', 'HTTPS', 'DNS', 'Telnet',\n",
      "       'SMTP', 'SSH', 'IRC', 'TCP', 'UDP', 'DHCP', 'ARP', 'ICMP', 'IPv', 'LLC',\n",
      "       'Tot sum', 'Min', 'Max', 'AVG', 'Std', 'Tot size', 'IAT', 'Number',\n",
      "       'Magnitue', 'Radius', 'Covariance', 'Variance', 'Weight', 'Label'],\n",
      "      dtype='object')\n",
      "Summary of dataset info\n",
      "<bound method DataFrame.info of               ID  flow_duration  Header_Length  Protocol type  Duration  \\\n",
      "0        2696539       4.789412         108.00           6.00     64.00   \n",
      "1        1487915      89.817633      122260.90           8.70    113.70   \n",
      "2        2033215    1496.467448      211355.40           8.20     88.70   \n",
      "3         131421       5.496918     2572540.10           5.50     64.00   \n",
      "4        2442255       0.161940          21.56           2.06     63.01   \n",
      "...          ...            ...            ...            ...       ...   \n",
      "1560105  2030322       0.000000          54.00           6.00       NaN   \n",
      "1560106   343287       0.000000          54.00           6.00     64.00   \n",
      "1560107  2076002       0.000000           0.00          47.00     64.00   \n",
      "1560108  2077018       0.024619       22675.00          17.00       NaN   \n",
      "1560109  1129777      28.088182      199379.70           7.10     97.20   \n",
      "\n",
      "               Rate         Srate  Drate  fin_flag_number  syn_flag_number  \\\n",
      "0          0.417588      0.417588    0.0              0.0              1.0   \n",
      "1         10.801200     10.801200    0.0              0.0              0.0   \n",
      "2               NaN     10.212064    NaN              0.0              0.0   \n",
      "3        258.758076    258.758076    0.0              0.0              0.0   \n",
      "4          7.361685      7.361685    0.0              0.0              0.0   \n",
      "...             ...           ...    ...              ...              ...   \n",
      "1560105    1.000009      1.000009    0.0              0.0              0.0   \n",
      "1560106   13.230220     13.230220    0.0              1.0              0.0   \n",
      "1560107    1.200522      1.200522    0.0              0.0              0.0   \n",
      "1560108         NaN  18407.017848    0.0              0.0              0.0   \n",
      "1560109   32.644331     32.644331    0.0              0.0              0.0   \n",
      "\n",
      "         ...          Std  Tot size           IAT  Number   Magnitue  \\\n",
      "0        ...     0.000000      54.0  8.297271e+07     9.5  10.392305   \n",
      "1        ...    45.260357     107.5  2.229638e-02     5.5  14.263331   \n",
      "2        ...    94.283337     131.4  1.414139e-02     5.5  18.980075   \n",
      "3        ...  1245.991343    1211.9  1.742196e-03     5.5  63.691494   \n",
      "4        ...   565.713979       NaN  8.324885e+07     9.5  40.225205   \n",
      "...      ...          ...       ...           ...     ...        ...   \n",
      "1560105  ...     0.000000      54.0  8.307224e+07     9.5  10.392305   \n",
      "1560106  ...     0.000000      54.0  8.334406e+07     9.5  10.392305   \n",
      "1560107  ...          NaN     578.0  8.358610e+07     9.5  34.000000   \n",
      "1560108  ...     0.000000      50.0  8.301549e+07     9.5  10.000000   \n",
      "1560109  ...   780.172801     336.0  1.665244e+08    13.5  30.015038   \n",
      "\n",
      "              Radius    Covariance  Variance  Weight                    Label  \n",
      "0           0.000000  0.000000e+00      0.00  141.55            DoS-SYN_Flood  \n",
      "1          64.007811  2.325051e+03      0.90   38.50            BenignTraffic  \n",
      "2         133.336774  9.939948e+03       NaN   38.50             Recon-OSScan  \n",
      "3                NaN  1.728192e+06      0.90   38.50         MITM-ArpSpoofing  \n",
      "4         800.119170  3.374647e+05      0.95  141.55  DDoS-ICMP_Fragmentation  \n",
      "...              ...           ...       ...     ...                      ...  \n",
      "1560105     0.000000  0.000000e+00       NaN  141.55           DDoS-TCP_Flood  \n",
      "1560106     0.000000  0.000000e+00      0.00  141.55         DDoS-RSTFINFlood  \n",
      "1560107     0.000000  0.000000e+00      0.00  141.55        Mirai-greip_flood  \n",
      "1560108     0.000000  0.000000e+00      0.00  141.55            DoS-UDP_Flood  \n",
      "1560109  1105.452406  6.132905e+05      1.00  244.60            BenignTraffic  \n",
      "\n",
      "[1560110 rows x 48 columns]>\n",
      "view dimensions of dataset\n",
      "ID                      0\n",
      "flow_duration           0\n",
      "Header_Length      155801\n",
      "Protocol type      155810\n",
      "Duration           156043\n",
      "Rate               156180\n",
      "Srate              156075\n",
      "Drate              156049\n",
      "fin_flag_number         0\n",
      "syn_flag_number         0\n",
      "rst_flag_number    156030\n",
      "psh_flag_number    156006\n",
      "ack_flag_number         0\n",
      "ece_flag_number    155889\n",
      "cwr_flag_number    156119\n",
      "ack_count          156078\n",
      "syn_count          156278\n",
      "fin_count               0\n",
      "urg_count               0\n",
      "rst_count               0\n",
      "HTTP               155993\n",
      "HTTPS              156399\n",
      "DNS                     0\n",
      "Telnet             156044\n",
      "SMTP               155832\n",
      "SSH                156261\n",
      "IRC                     0\n",
      "TCP                156010\n",
      "UDP                     0\n",
      "DHCP                    0\n",
      "ARP                156189\n",
      "ICMP               155988\n",
      "IPv                     0\n",
      "LLC                     0\n",
      "Tot sum            155800\n",
      "Min                156138\n",
      "Max                155961\n",
      "AVG                     0\n",
      "Std                156058\n",
      "Tot size           155938\n",
      "IAT                156215\n",
      "Number                  0\n",
      "Magnitue                0\n",
      "Radius             155906\n",
      "Covariance         156232\n",
      "Variance           156106\n",
      "Weight                  0\n",
      "Label                   0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check dataset\n",
    "df = pd.read_csv('data/train_.csv')\n",
    "df.head()\n",
    "print(\"Dataset column\")\n",
    "print(df.columns)\n",
    "print(\"Summary of dataset info\")\n",
    "print(df.info)\n",
    "print(\"view dimensions of dataset\")\n",
    "df.shape\n",
    "\n",
    "# for col in df.columns:\n",
    "#   if df[col].dtype != 'object':  # Exclude non-numeric columns\n",
    "#     min_val = df[col].min()\n",
    "#     max_val = df[col].max()\n",
    "#     print(f\"Column: {col}\")\n",
    "#     print(f\"Minimum: {min_val}\")\n",
    "#     print(f\"Maximum: {max_val}\")\n",
    "#     print()\n",
    "\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see some cell have null value, we can not drop which rows have null cell because it to much. So we just fill all null value = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID                      0\n",
      "flow_duration           0\n",
      "Header_Length      155801\n",
      "Protocol type      155810\n",
      "Duration           156043\n",
      "Rate               156180\n",
      "Srate              156075\n",
      "Drate              156049\n",
      "fin_flag_number         0\n",
      "syn_flag_number         0\n",
      "rst_flag_number    156030\n",
      "psh_flag_number    156006\n",
      "ack_flag_number         0\n",
      "ece_flag_number    155889\n",
      "cwr_flag_number    156119\n",
      "ack_count          156078\n",
      "syn_count          156278\n",
      "fin_count               0\n",
      "urg_count               0\n",
      "rst_count               0\n",
      "HTTP               155993\n",
      "HTTPS              156399\n",
      "DNS                     0\n",
      "Telnet             156044\n",
      "SMTP               155832\n",
      "SSH                156261\n",
      "IRC                     0\n",
      "TCP                156010\n",
      "UDP                     0\n",
      "DHCP                    0\n",
      "ARP                156189\n",
      "ICMP               155988\n",
      "IPv                     0\n",
      "LLC                     0\n",
      "Tot sum            155800\n",
      "Min                156138\n",
      "Max                155961\n",
      "AVG                     0\n",
      "Std                156058\n",
      "Tot size           155938\n",
      "IAT                156215\n",
      "Number                  0\n",
      "Magnitue                0\n",
      "Radius             155906\n",
      "Covariance         156232\n",
      "Variance           156106\n",
      "Weight                  0\n",
      "Label                   0\n",
      "dtype: int64\n",
      "ID                 0\n",
      "flow_duration      0\n",
      "Header_Length      0\n",
      "Protocol type      0\n",
      "Duration           0\n",
      "Rate               0\n",
      "Srate              0\n",
      "Drate              0\n",
      "fin_flag_number    0\n",
      "syn_flag_number    0\n",
      "rst_flag_number    0\n",
      "psh_flag_number    0\n",
      "ack_flag_number    0\n",
      "ece_flag_number    0\n",
      "cwr_flag_number    0\n",
      "ack_count          0\n",
      "syn_count          0\n",
      "fin_count          0\n",
      "urg_count          0\n",
      "rst_count          0\n",
      "HTTP               0\n",
      "HTTPS              0\n",
      "DNS                0\n",
      "Telnet             0\n",
      "SMTP               0\n",
      "SSH                0\n",
      "IRC                0\n",
      "TCP                0\n",
      "UDP                0\n",
      "DHCP               0\n",
      "ARP                0\n",
      "ICMP               0\n",
      "IPv                0\n",
      "LLC                0\n",
      "Tot sum            0\n",
      "Min                0\n",
      "Max                0\n",
      "AVG                0\n",
      "Std                0\n",
      "Tot size           0\n",
      "IAT                0\n",
      "Number             0\n",
      "Magnitue           0\n",
      "Radius             0\n",
      "Covariance         0\n",
      "Variance           0\n",
      "Weight             0\n",
      "Label              0\n",
      "dtype: int64\n",
      "0\n",
      "['DoS-SYN_Flood', 'BenignTraffic', 'Recon-OSScan', 'MITM-ArpSpoofing', 'DDoS-ICMP_Fragmentation', 'Mirai-udpplain', 'DoS-TCP_Flood', 'DDoS-ACK_Fragmentation', 'DDoS-UDP_Fragmentation', 'Mirai-greeth_flood', 'DDoS-RSTFINFlood', 'Mirai-greip_flood', 'DDoS-TCP_Flood', 'DNS_Spoofing', 'DoS-HTTP_Flood', 'Recon-PortScan', 'VulnerabilityScan', 'DDoS-ICMP_Flood', 'DDoS-SynonymousIP_Flood', 'Recon-PingSweep', 'DDoS-SYN_Flood', 'DDoS-SlowLoris', 'DDoS-UDP_Flood', 'DoS-UDP_Flood', 'Recon-HostDiscovery', 'DDoS-PSHACK_Flood', 'DictionaryBruteForce', 'XSS', 'DDoS-HTTP_Flood', 'CommandInjection', 'BrowserHijacking', 'Uploading_Attack', 'Backdoor_Malware', 'SqlInjection']\n"
     ]
    }
   ],
   "source": [
    "# Fill all null data as -1\n",
    "print(df.isnull().sum())\n",
    "data_n_null = df.fillna(-1, inplace=False)\n",
    "print(data_n_null.isnull().sum())\n",
    "\n",
    "data_n_null.head()\n",
    "print(data_n_null.duplicated().sum())\n",
    "print(data_n_null['Label'].unique().tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For tracking during training, we using MLflow. The software defined by container in mlflow folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set mlflow as tracking server\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model\n",
    "We train with some model with these steps\n",
    "- We training with small part of dataset (0.2 or 0.3): dataset_frac\n",
    "- We log artifacts, we see some column less contribute in  Feature Importance Score, so we delete it\n",
    "- We train with full dataset, verify droped column is correct and need modify or not\n",
    "- We use RandomizedSearchCV to search parameter\n",
    "- We save best parameter to mlflow. With mlflow.sklearn.autolog, model and its metrics was save to model registry. We just download it and use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n",
    "1. We training with small dataset, use RandomizedSearchCV and get this artifacts\n",
    "- Run Overview\n",
    "    ![decisiontree-run-overview](./imgs/decision_tree/run_overview.png)\n",
    "\n",
    "- Feature importance\n",
    "    ![decisiontree-feature-importance](./imgs/decision_tree/feature_importance.png)\n",
    "\n",
    "- Confusion matrix\n",
    "\n",
    "    ![decisiontree-confusion-matrix](./imgs/decision_tree/training_confusion_matrix.png)\n",
    "\n",
    "2. Then, we train with full dataset. Best model save on models folder. Picture bellow show some difference 2 runs.\n",
    "\n",
    "![](./imgs/decision_tree/difference.png) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] END criterion=log_loss, max_depth=60, max_features=2, min_samples_leaf=15, min_samples_split=9; total time=   6.9s\n",
      "[CV] END criterion=log_loss, max_depth=60, max_features=2, min_samples_leaf=15, min_samples_split=9; total time=   6.4s\n",
      "[CV] END criterion=log_loss, max_depth=60, max_features=2, min_samples_leaf=15, min_samples_split=9; total time=   6.7s\n",
      "[CV] END criterion=log_loss, max_depth=60, max_features=2, min_samples_leaf=15, min_samples_split=9; total time=   5.8s\n",
      "[CV] END criterion=log_loss, max_depth=60, max_features=2, min_samples_leaf=15, min_samples_split=9; total time=   6.9s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=14, min_samples_leaf=8, min_samples_split=9; total time=  16.2s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=14, min_samples_leaf=8, min_samples_split=9; total time=  16.5s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=14, min_samples_leaf=8, min_samples_split=9; total time=  15.9s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=14, min_samples_leaf=8, min_samples_split=9; total time=  15.1s\n",
      "[CV] END criterion=gini, max_depth=40, max_features=14, min_samples_leaf=8, min_samples_split=9; total time=  15.9s\n",
      "[CV] END criterion=entropy, max_depth=100, max_features=17, min_samples_leaf=13, min_samples_split=9; total time=  25.6s\n",
      "[CV] END criterion=entropy, max_depth=100, max_features=17, min_samples_leaf=13, min_samples_split=9; total time=  26.1s\n",
      "[CV] END criterion=entropy, max_depth=100, max_features=17, min_samples_leaf=13, min_samples_split=9; total time=  24.5s\n",
      "[CV] END criterion=entropy, max_depth=100, max_features=17, min_samples_leaf=13, min_samples_split=9; total time=  25.9s\n",
      "[CV] END criterion=entropy, max_depth=100, max_features=17, min_samples_leaf=13, min_samples_split=9; total time=  27.2s\n",
      "[CV] END criterion=entropy, max_depth=90, max_features=23, min_samples_leaf=13, min_samples_split=3; total time=  33.0s\n",
      "[CV] END criterion=entropy, max_depth=90, max_features=23, min_samples_leaf=13, min_samples_split=3; total time=  34.8s\n",
      "[CV] END criterion=entropy, max_depth=90, max_features=23, min_samples_leaf=13, min_samples_split=3; total time=  34.0s\n",
      "[CV] END criterion=entropy, max_depth=90, max_features=23, min_samples_leaf=13, min_samples_split=3; total time=  34.2s\n",
      "[CV] END criterion=entropy, max_depth=90, max_features=23, min_samples_leaf=13, min_samples_split=3; total time=  33.5s\n",
      "[CV] END criterion=entropy, max_depth=110, max_features=30, min_samples_leaf=13, min_samples_split=7; total time=  38.0s\n",
      "[CV] END criterion=entropy, max_depth=110, max_features=30, min_samples_leaf=13, min_samples_split=7; total time=  38.7s\n",
      "[CV] END criterion=entropy, max_depth=110, max_features=30, min_samples_leaf=13, min_samples_split=7; total time=  38.1s\n",
      "[CV] END criterion=entropy, max_depth=110, max_features=30, min_samples_leaf=13, min_samples_split=7; total time=  38.1s\n",
      "[CV] END criterion=entropy, max_depth=110, max_features=30, min_samples_leaf=13, min_samples_split=7; total time=  38.2s\n",
      "[CV] END criterion=log_loss, max_depth=20, max_features=2, min_samples_leaf=13, min_samples_split=9; total time=   7.1s\n",
      "[CV] END criterion=log_loss, max_depth=20, max_features=2, min_samples_leaf=13, min_samples_split=9; total time=   6.4s\n",
      "[CV] END criterion=log_loss, max_depth=20, max_features=2, min_samples_leaf=13, min_samples_split=9; total time=   5.5s\n",
      "[CV] END criterion=log_loss, max_depth=20, max_features=2, min_samples_leaf=13, min_samples_split=9; total time=   6.0s\n",
      "[CV] END criterion=log_loss, max_depth=20, max_features=2, min_samples_leaf=13, min_samples_split=9; total time=   5.9s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=26, min_samples_leaf=13, min_samples_split=3; total time=  22.8s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=26, min_samples_leaf=13, min_samples_split=3; total time=  22.6s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=26, min_samples_leaf=13, min_samples_split=3; total time=  23.3s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=26, min_samples_leaf=13, min_samples_split=3; total time=  23.1s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=26, min_samples_leaf=13, min_samples_split=3; total time=  23.1s\n",
      "[CV] END criterion=entropy, max_depth=60, max_features=23, min_samples_leaf=18, min_samples_split=3; total time=  32.6s\n",
      "[CV] END criterion=entropy, max_depth=60, max_features=23, min_samples_leaf=18, min_samples_split=3; total time=  33.4s\n",
      "[CV] END criterion=entropy, max_depth=60, max_features=23, min_samples_leaf=18, min_samples_split=3; total time=  32.5s\n",
      "[CV] END criterion=entropy, max_depth=60, max_features=23, min_samples_leaf=18, min_samples_split=3; total time=  34.5s\n",
      "[CV] END criterion=entropy, max_depth=60, max_features=23, min_samples_leaf=18, min_samples_split=3; total time=  34.0s\n",
      "[CV] END criterion=entropy, max_depth=110, max_features=11, min_samples_leaf=18, min_samples_split=8; total time=  20.4s\n",
      "[CV] END criterion=entropy, max_depth=110, max_features=11, min_samples_leaf=18, min_samples_split=8; total time=  19.0s\n",
      "[CV] END criterion=entropy, max_depth=110, max_features=11, min_samples_leaf=18, min_samples_split=8; total time=  18.2s\n",
      "[CV] END criterion=entropy, max_depth=110, max_features=11, min_samples_leaf=18, min_samples_split=8; total time=  18.8s\n",
      "[CV] END criterion=entropy, max_depth=110, max_features=11, min_samples_leaf=18, min_samples_split=8; total time=  18.7s\n",
      "[CV] END criterion=log_loss, max_depth=100, max_features=8, min_samples_leaf=15, min_samples_split=8; total time=  16.5s\n",
      "[CV] END criterion=log_loss, max_depth=100, max_features=8, min_samples_leaf=15, min_samples_split=8; total time=  14.5s\n",
      "[CV] END criterion=log_loss, max_depth=100, max_features=8, min_samples_leaf=15, min_samples_split=8; total time=  14.7s\n",
      "[CV] END criterion=log_loss, max_depth=100, max_features=8, min_samples_leaf=15, min_samples_split=8; total time=  14.2s\n",
      "[CV] END criterion=log_loss, max_depth=100, max_features=8, min_samples_leaf=15, min_samples_split=8; total time=  14.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/12/02 23:36:13 INFO mlflow.sklearn.utils: Logging the 5 best runs, 5 runs will be omitted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run sassy-lamb-941 at: http://localhost:5000/#/experiments/5/runs/98207c11a7b045b3819468a3f8455d97\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/5\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Classification: https://gist.github.com/pb111/af439e4affb1dd94879579cfd6793770\n",
    "mlflow.set_experiment(\"decision_tree\")\n",
    "\n",
    "tags = {\n",
    "    \"dataset_frac\": 1.0,\n",
    "    \"random_state\": 42,\n",
    "    \"test_size\" : 0.2,\n",
    "    # \"droped_column\" : ['ID'],\n",
    "    \"droped_column\" : ['ID','IPv','DNS','IRC','DHCP','ARP','SMTP','cwr_flag_number','ece_flag_number','Telnet','Drate','psh_flag_number','rst_flag_number','LLC', 'TCP','SSH','HTTPS','ack_flag_number','Std','Tot size', 'ack_count'],\n",
    "    \"author\": \"Son Nguyen\",\n",
    "    \"parameter\" : {\n",
    "        \"max_features\" : [int(x) for x in np.linspace(2, 30, num = 10)],\n",
    "        \"criterion\" : ['gini','entropy','log_loss'],\n",
    "        \"max_depth\" : [int(x) for x in np.linspace(10, 110, num = 11)],\n",
    "        \"min_samples_split\" : [int(x) for x in np.random.randint(2, 10, 5)],\n",
    "        \"min_samples_leaf\" : [int(x) for x in np.random.randint(2, 20, 5)]\n",
    "    }\n",
    "}\n",
    "tags['parameter']['max_depth'].append(None)\n",
    "\n",
    "# Prepare data\n",
    "data = data_n_null.drop(columns=tags['droped_column'])\n",
    "data_sample = data.sample(frac=tags['dataset_frac'])\n",
    "X = data_sample.drop(columns=['Label'])\n",
    "y = data_sample['Label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = tags['test_size'], random_state = tags['random_state'])\n",
    "\n",
    "# Pick model\n",
    "tree = DecisionTreeClassifier(random_state=tags['random_state'])\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=tree,\n",
    "    param_distributions =tags['parameter'],\n",
    "    cv=5,\n",
    "    random_state= tags['random_state'],\n",
    "    scoring='f1_weighted',  \n",
    "    verbose=2,\n",
    "    error_score='raise',\n",
    "    n_jobs=-1  # Use all available CPU cores\n",
    ")\n",
    "\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.sklearn.autolog()\n",
    "    random_search.fit(X_train, y_train)\n",
    "    best_model = random_search.best_estimator_\n",
    "    best_params = random_search.best_params_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Create some artifact\n",
    "    feature_scores = pd.Series(best_model.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    sns.barplot(x=feature_scores, y=feature_scores.index)\n",
    "    plt.xlabel('Feature Importance Score')\n",
    "    plt.ylabel('Features')\n",
    "    plt.title(\"Visualizing Important Features\")\n",
    "    feature_importance_plot = \"feature_importance.png\"\n",
    "    plt.savefig(feature_importance_plot, bbox_inches='tight')\n",
    "\n",
    "    report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "    report_df = pd.DataFrame(report_dict).transpose()\n",
    "    report_df.reset_index(inplace=True)\n",
    "    report_df.rename(columns={\"index\": \"Attack Type\"}, inplace=True)\n",
    "    report_filename = \"classification_report.csv\"\n",
    "    report_df.to_csv(report_filename, index=False)\n",
    "\n",
    "    mlflow.log_artifact(feature_importance_plot)\n",
    "    mlflow.log_artifact(report_filename)\n",
    "    os.remove(feature_importance_plot)\n",
    "    os.remove(report_filename)\n",
    "\n",
    "    for key, value in tags.items():\n",
    "        if key != \"parameter\":\n",
    "            mlflow.set_tag(key, value)\n",
    "    mlflow.set_tag(\"best_param\", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "Now, we do same thing with GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] END ................var_smoothing=2.848035868435799e-08; total time=   5.9s\n",
      "[CV] END ................var_smoothing=2.848035868435799e-08; total time=   5.8s\n",
      "[CV] END ................var_smoothing=2.848035868435799e-08; total time=   5.9s\n",
      "[CV] END ................var_smoothing=2.848035868435799e-08; total time=   5.9s\n",
      "[CV] END ................var_smoothing=2.848035868435799e-08; total time=   6.0s\n",
      "[CV] END ...............var_smoothing=1.5199110829529332e-05; total time=   5.9s\n",
      "[CV] END ...............var_smoothing=1.5199110829529332e-05; total time=   6.0s\n",
      "[CV] END ...............var_smoothing=1.5199110829529332e-05; total time=   6.1s\n",
      "[CV] END ...............var_smoothing=1.5199110829529332e-05; total time=   5.9s\n",
      "[CV] END ...............var_smoothing=1.5199110829529332e-05; total time=   6.0s\n",
      "[CV] END ...............var_smoothing=4.3287612810830526e-07; total time=   5.9s\n",
      "[CV] END ...............var_smoothing=4.3287612810830526e-07; total time=   6.0s\n",
      "[CV] END ...............var_smoothing=4.3287612810830526e-07; total time=   6.0s\n",
      "[CV] END ...............var_smoothing=4.3287612810830526e-07; total time=   6.0s\n",
      "[CV] END ...............var_smoothing=4.3287612810830526e-07; total time=   5.9s\n",
      "[CV] END ................var_smoothing=8.111308307896872e-05; total time=   5.9s\n",
      "[CV] END ................var_smoothing=8.111308307896872e-05; total time=   6.0s\n",
      "[CV] END ................var_smoothing=8.111308307896872e-05; total time=   5.9s\n",
      "[CV] END ................var_smoothing=8.111308307896872e-05; total time=   5.9s\n",
      "[CV] END ................var_smoothing=8.111308307896872e-05; total time=   5.8s\n",
      "[CV] END ...............................var_smoothing=0.0001; total time=   6.0s\n",
      "[CV] END ...............................var_smoothing=0.0001; total time=   5.9s\n",
      "[CV] END ...............................var_smoothing=0.0001; total time=   5.8s\n",
      "[CV] END ...............................var_smoothing=0.0001; total time=   5.9s\n",
      "[CV] END ...............................var_smoothing=0.0001; total time=   5.9s\n",
      "[CV] END ................var_smoothing=0.0002848035868435802; total time=   6.0s\n",
      "[CV] END ................var_smoothing=0.0002848035868435802; total time=   5.9s\n",
      "[CV] END ................var_smoothing=0.0002848035868435802; total time=   6.0s\n",
      "[CV] END ................var_smoothing=0.0002848035868435802; total time=   5.9s\n",
      "[CV] END ................var_smoothing=0.0002848035868435802; total time=   5.9s\n",
      "[CV] END .................................var_smoothing=0.01; total time=   6.0s\n",
      "[CV] END .................................var_smoothing=0.01; total time=   5.9s\n",
      "[CV] END .................................var_smoothing=0.01; total time=   6.1s\n",
      "[CV] END .................................var_smoothing=0.01; total time=   6.0s\n",
      "[CV] END .................................var_smoothing=0.01; total time=   5.8s\n",
      "[CV] END ................var_smoothing=5.336699231206302e-08; total time=   5.9s\n",
      "[CV] END ................var_smoothing=5.336699231206302e-08; total time=   5.9s\n",
      "[CV] END ................var_smoothing=5.336699231206302e-08; total time=   5.9s\n",
      "[CV] END ................var_smoothing=5.336699231206302e-08; total time=   6.0s\n",
      "[CV] END ................var_smoothing=5.336699231206302e-08; total time=   6.0s\n",
      "[CV] END ..................var_smoothing=0.12328467394420659; total time=   5.9s\n",
      "[CV] END ..................var_smoothing=0.12328467394420659; total time=   6.0s\n",
      "[CV] END ..................var_smoothing=0.12328467394420659; total time=   6.1s\n",
      "[CV] END ..................var_smoothing=0.12328467394420659; total time=   5.9s\n",
      "[CV] END ..................var_smoothing=0.12328467394420659; total time=   5.9s\n",
      "[CV] END ..................................var_smoothing=1.0; total time=   5.8s\n",
      "[CV] END ..................................var_smoothing=1.0; total time=   5.9s\n",
      "[CV] END ..................................var_smoothing=1.0; total time=   6.0s\n",
      "[CV] END ..................................var_smoothing=1.0; total time=   6.1s\n",
      "[CV] END ..................................var_smoothing=1.0; total time=   6.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\project\\mlsec\\final\\project\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "2024/12/03 19:53:20 INFO mlflow.sklearn.utils: Logging the 5 best runs, 5 runs will be omitted.\n",
      "d:\\project\\mlsec\\final\\project\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\project\\mlsec\\final\\project\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run shivering-cat-250 at: http://localhost:5000/#/experiments/7/runs/331f50ef4eba4125b34fe15666892090\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\project\\mlsec\\final\\project\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# # Naive Bayes https://gist.github.com/pb111/9e3816d2584a85ef7bff8d70bed20b1b\n",
    "mlflow.set_experiment(\"naive-bayes\")\n",
    "\n",
    "tags = {\n",
    "    \"dataset_frac\": 1.0,\n",
    "    \"random_state\": 42,\n",
    "    \"test_size\" : 0.2,\n",
    "    \"droped_column\" : ['ID'],\n",
    "    # \"droped_column\" : ['ID','IRC','DHCP','LLC','IPv','DNS','ece_flag_number','Drate','SMTP','Telnet','cwr_flag_number','ARP','SSH'],\n",
    "    \"author\": \"Son Nguyen\",\n",
    "    \"parameter\" : {\n",
    "        # \"var_smoothing\": np.logspace(-9, -1, num=50)\n",
    "        \"var_smoothing\": np.logspace(0,-9, num=100)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Prepare data\n",
    "data = data_n_null.drop(columns=tags['droped_column'])\n",
    "data_sample = data.sample(frac=tags['dataset_frac'])\n",
    "X = data_sample.drop(columns=['Label'])\n",
    "y = data_sample['Label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = tags['test_size'], random_state = tags['random_state'])\n",
    "\n",
    "# Pick model\n",
    "gnb = GaussianNB()\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=gnb,\n",
    "    param_distributions =tags['parameter'],\n",
    "    cv=5,\n",
    "    random_state= tags['random_state'],\n",
    "    scoring='f1_weighted',  \n",
    "    verbose=2,\n",
    "    error_score='raise',\n",
    "    n_jobs=None\n",
    ")\n",
    "\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.sklearn.autolog()\n",
    "    random_search.fit(X_train, y_train)\n",
    "    best_model = random_search.best_estimator_\n",
    "    best_params = random_search.best_params_\n",
    "    y_pred = best_model.predict(X_test)\n",
    " \n",
    "\n",
    "    # Create some artifact\n",
    "    # perm_importance = permutation_importance(best_model, X_train, y_train, scoring=\"f1_weighted\", n_repeats=10, random_state=42)\n",
    "    # feature_scores = pd.Series(perm_importance.importances_mean, index=X_train.columns).sort_values(ascending=False)\n",
    "    # plt.figure(figsize=(20, 20))\n",
    "    # sns.barplot(x=feature_scores, y=feature_scores.index)\n",
    "    # plt.xlabel('Feature Importance Score')\n",
    "    # plt.ylabel('Features')\n",
    "    # plt.title(\"Visualizing Important Features\")\n",
    "    # feature_importance_plot = \"feature_importance.png\"\n",
    "    # plt.savefig(feature_importance_plot, bbox_inches='tight')\n",
    "\n",
    "    report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "    report_df = pd.DataFrame(report_dict).transpose()\n",
    "    report_df.reset_index(inplace=True)\n",
    "    report_df.rename(columns={\"index\": \"Attack Type\"}, inplace=True)\n",
    "    report_filename = \"classification_report.csv\"\n",
    "    report_df.to_csv(report_filename, index=False)\n",
    "\n",
    "    # mlflow.log_artifact(feature_importance_plot)\n",
    "    mlflow.log_artifact(report_filename)\n",
    "    # os.remove(feature_importance_plot)\n",
    "    os.remove(report_filename)\n",
    "\n",
    "    for key, value in tags.items():\n",
    "        if key != \"parameter\":\n",
    "            mlflow.set_tag(key, value)\n",
    "    mlflow.set_tag(\"best_param\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run blushing-lark-657 at: http://localhost:5000/#/experiments/3/runs/fae831c3dc3642b8ae809d7a3553b792\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/3\n"
     ]
    }
   ],
   "source": [
    "# # # # Random forest follow this: https://gist.github.com/pb111/88545fa33780928694388779af23bf58\n",
    "# # # Turning hyper parameter follow this: https://www.geeksforgeeks.org/random-forest-hyperparameter-tuning-in-python/\n",
    "mlflow.set_experiment(\"random-forest\")\n",
    "\n",
    "tags = {\n",
    "    \"dataset_frac\": 0.2,\n",
    "    \"random_state\": 42,\n",
    "    \"test_size\" : 0.2,\n",
    "    # \"droped_column\" : ['ID'],\n",
    "    \"droped_column\" : ['ID','IRC','DHCP','LLC','IPv','DNS','ece_flag_number','Drate','SMTP','Telnet','cwr_flag_number','ARP','SSH'],\n",
    "    \"author\": \"Son Nguyen\",\n",
    "    \"parameter\" : {\n",
    "        \"n_estimators\" : [int(x) for x in np.linspace(start = 50, stop = 1000, num = 10)],\n",
    "        \"criterion\" : ['gini','entropy','log_loss'],\n",
    "        \"max_depth\" : [int(x) for x in np.linspace(10, 1000, num = 20)],\n",
    "        \"min_samples_split\" : [int(x) for x in np.random.randint(2, 10, 5)],\n",
    "        \"min_samples_leaf\" : [int(x) for x in np.random.randint(1, 20, 5)],\n",
    "        \"max_features\" : [int(x) for x in np.random.randint(2, 30, 10)],\n",
    "        \"max_leaf_nodes\" : [int(x) for x in np.linspace(10, 100, num = 13)],\n",
    "        \"bootstrap\" : [True],\n",
    "        \"max_samples\" : np.random.rand(5),\n",
    "        \"class_weight\" : ['balanced','balanced_subsample',None]\n",
    "    }\n",
    "}\n",
    "tags['parameter']['max_depth'].append(None)\n",
    "tags['parameter']['max_leaf_nodes'].append(None)\n",
    "\n",
    "# Prepare data\n",
    "data = data_n_null.drop(columns=tags['droped_column'])\n",
    "data_sample = data.sample(frac=tags['dataset_frac'])\n",
    "X = data_sample.drop(columns=['Label'])\n",
    "y = data_sample['Label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = tags['test_size'], random_state = tags['random_state'])\n",
    "\n",
    "# Pick model\n",
    "forest = RandomForestClassifier(random_state = tags['random_state'])\n",
    "# random_search = RandomizedSearchCV(\n",
    "#     estimator=forest,\n",
    "#     param_distributions = tags['parameter'],\n",
    "#     cv=5,\n",
    "#     random_state= tags['random_state'],\n",
    "#     scoring='f1_weighted',  \n",
    "#     verbose=2,\n",
    "#     error_score='raise',\n",
    "#     n_jobs=None  # Use all available CPU cores\n",
    "# )\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.sklearn.autolog()\n",
    "    forest.fit(X_train, y_train)\n",
    "    # best_model = random_search.best_estimator_\n",
    "    # best_params = random_search.best_params_\n",
    "    y_pred = forest.predict(X_test)\n",
    "\n",
    "    # Create some artifact\n",
    "    feature_scores = pd.Series(forest.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    sns.barplot(x=feature_scores, y=feature_scores.index)\n",
    "    plt.xlabel('Feature Importance Score')\n",
    "    plt.ylabel('Features')\n",
    "    plt.title(\"Visualizing Important Features\")\n",
    "    feature_importance_plot = \"feature_importance.png\"\n",
    "    plt.savefig(feature_importance_plot, bbox_inches='tight')\n",
    "\n",
    "    report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "    report_df = pd.DataFrame(report_dict).transpose()\n",
    "    report_df.reset_index(inplace=True)\n",
    "    report_df.rename(columns={\"index\": \"Attack Type\"}, inplace=True)\n",
    "    report_filename = \"classification_report.csv\"\n",
    "    report_df.to_csv(report_filename, index=False)\n",
    "\n",
    "    mlflow.log_artifact(feature_importance_plot)\n",
    "    mlflow.log_artifact(report_filename)\n",
    "    os.remove(feature_importance_plot)\n",
    "    os.remove(report_filename)\n",
    "\n",
    "    for key, value in tags.items():\n",
    "        if key != \"parameter\":\n",
    "            mlflow.set_tag(key, value)\n",
    "    # mlflow.set_tag(\"best_param\", best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
